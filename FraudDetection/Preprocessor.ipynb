{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f345412b",
   "metadata": {},
   "source": [
    "First, we need to preprocess the data before feeding it to the LSTM, since the LSTM works on a timing window basis with the memory of previous transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1859c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from requests import get\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    FunctionTransformer,\n",
    "    MinMaxScaler,\n",
    "    LabelBinarizer,\n",
    ")\n",
    "from sklearn_pandas import DataFrameMapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9d0928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio Fraud/Non-Fraud: 0.00122169500749739\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999008.0</td>\n",
       "      <td>992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998768.0</td>\n",
       "      <td>1232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998803.0</td>\n",
       "      <td>1197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998856.0</td>\n",
       "      <td>1144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998891.0</td>\n",
       "      <td>1109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998675.0</td>\n",
       "      <td>1325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998830.0</td>\n",
       "      <td>1170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998686.0</td>\n",
       "      <td>1314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998749.0</td>\n",
       "      <td>1251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998721.0</td>\n",
       "      <td>1279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999017.0</td>\n",
       "      <td>983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998667.0</td>\n",
       "      <td>1333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998623.0</td>\n",
       "      <td>1377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998730.0</td>\n",
       "      <td>1270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998835.0</td>\n",
       "      <td>1165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998893.0</td>\n",
       "      <td>1107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998559.0</td>\n",
       "      <td>1441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998826.0</td>\n",
       "      <td>1174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998610.0</td>\n",
       "      <td>1390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998659.0</td>\n",
       "      <td>1341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998835.0</td>\n",
       "      <td>1165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998700.0</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998730.0</td>\n",
       "      <td>1270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998985.0</td>\n",
       "      <td>1015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>386487.0</td>\n",
       "      <td>413.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         No     Yes\n",
       "0  999008.0   992.0\n",
       "0  998768.0  1232.0\n",
       "0  998803.0  1197.0\n",
       "0  998856.0  1144.0\n",
       "0  998891.0  1109.0\n",
       "0  998675.0  1325.0\n",
       "0  998830.0  1170.0\n",
       "0  998686.0  1314.0\n",
       "0  998749.0  1251.0\n",
       "0  998721.0  1279.0\n",
       "0  999017.0   983.0\n",
       "0  998667.0  1333.0\n",
       "0  998623.0  1377.0\n",
       "0  998730.0  1270.0\n",
       "0  998835.0  1165.0\n",
       "0  998893.0  1107.0\n",
       "0  998559.0  1441.0\n",
       "0  998826.0  1174.0\n",
       "0  998610.0  1390.0\n",
       "0  998659.0  1341.0\n",
       "0  998835.0  1165.0\n",
       "0  998700.0  1300.0\n",
       "0  998730.0  1270.0\n",
       "0  998985.0  1015.0\n",
       "0  386487.0   413.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = pd.DataFrame({\"No\": [], \"Yes\": []})\n",
    "df_nf = pd.DataFrame()\n",
    "df_f = pd.DataFrame()\n",
    "\n",
    "# Read and process the CSV in chunks\n",
    "with pd.read_csv(\"card_transaction.v1.csv\", chunksize=1_000_000) as reader:\n",
    "    for chunk in reader:\n",
    "        # Sample 5% of non-fraud rows\n",
    "        df_nf = pd.concat([df_nf, chunk[chunk[\"Is Fraud?\"] == \"No\"].sample(frac=0.05)])\n",
    "        # Keep all fraud rows\n",
    "        df_f = pd.concat([df_f, chunk[chunk[\"Is Fraud?\"] == \"Yes\"]])\n",
    "        # Track counts for ratio statistics\n",
    "        vc = chunk[\"Is Fraud?\"].value_counts()\n",
    "        new = pd.DataFrame({\"No\": [vc.get(\"No\", 0)], \"Yes\": [vc.get(\"Yes\", 0)]})\n",
    "        dist = pd.concat([dist, new])\n",
    "\n",
    "# Save the results\n",
    "df_nf.to_csv(\"card_transactions_non-frauds.csv\", index=False)\n",
    "df_f.to_csv(\"card_transactions_frauds.csv\", index=False)\n",
    "print(f\"Ratio Fraud/Non-Fraud: {dist['Yes'].sum() / dist['No'].sum()}\")\n",
    "dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa99bfc",
   "metadata": {},
   "source": [
    "The transaction features need to be encoded as the LSTM requires numerical input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815acf64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shape: (1000000, 81)\n",
      "First few rows:\n",
      " [[0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         1.         1.         1.\n",
      "  1.         1.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.26837517 0.55490584]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         1.         1.\n",
      "  1.         1.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.26837684 0.41348956]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         1.         1.\n",
      "  1.         1.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.26848975 0.54265   ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         1.         1.\n",
      "  1.         1.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.26854406 0.5504781 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         1.         1.         1.\n",
      "  1.         1.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.26860433 0.52688969]]\n",
      "Feature names: ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer, OneHotEncoder, MinMaxScaler, LabelBinarizer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def fraud_encoder(X):\n",
    "    # Accepts DataFrame or Series\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.iloc[:, 0]\n",
    "    return np.where(X == \"Yes\", 1, 0).reshape(-1, 1)\n",
    "\n",
    "def amt_encoder(X):\n",
    "    # Accepts DataFrame or Series\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.iloc[:, 0]\n",
    "    amt = (\n",
    "        X.astype(str).str.replace(\"$\", \"\", regex=False)\n",
    "        .astype(float)\n",
    "        .map(lambda amt: max(1, amt))\n",
    "        .map(math.log)\n",
    "    )\n",
    "    return np.array(amt).reshape(-1, 1)\n",
    "\n",
    "def decimal_encoder(X, length=5):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.iloc[:, 0]\n",
    "    X = X.astype(str).str.replace(r'\\D', '', regex=True)\n",
    "    X = X.replace('', '0').astype(int)\n",
    "    arr = []\n",
    "    for i in range(length):\n",
    "        arr.append(np.mod(X, 10))\n",
    "        X = np.floor_divide(X, 10)\n",
    "    return np.column_stack(arr)\n",
    "\n",
    "def time_encoder(df):\n",
    "    X_hm = df[\"Time\"].str.split(\":\", expand=True)\n",
    "    d = pd.to_datetime(\n",
    "        dict(\n",
    "            year=df[\"Year\"], month=df[\"Month\"], day=df[\"Day\"], hour=X_hm[0], minute=X_hm[1]\n",
    "        )\n",
    "    ).astype(int)\n",
    "    return np.array(d).reshape(-1, 1)\n",
    "\n",
    "def binarizer_func(x):\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        x = x.iloc[:, 0]\n",
    "    return LabelBinarizer().fit_transform(x.astype(str)).reshape(-1, 1)\n",
    "\n",
    "# Load data and define the columns\n",
    "\n",
    "tdf = pd.read_csv(\"./card_transaction.v1.csv\", nrows=1_000_000)\n",
    "tdf[\"Merchant Name\"] = tdf[\"Merchant Name\"].astype(str)\n",
    "tdf.drop([\"MCC\", \"Zip\", \"Merchant State\"], axis=1, inplace=True)\n",
    "tdf.sort_values(by=[\"User\", \"Card\"], inplace=True)\n",
    "tdf.reset_index(inplace=True, drop=True)\n",
    "\n",
    "fraud_col = \"Is Fraud?\"\n",
    "merchant_name_col = \"Merchant Name\"\n",
    "merchant_city_col = \"Merchant City\"\n",
    "chip_col = \"Use Chip\"\n",
    "errors_col = \"Errors?\"\n",
    "time_cols = [\"Year\", \"Month\", \"Day\", \"Time\"]\n",
    "amt_col = \"Amount\"\n",
    "\n",
    "# Preprocessor pipeline\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Target label\n",
    "        (\"fraud\", FunctionTransformer(fraud_encoder, validate=False), [fraud_col]),\n",
    "\n",
    "        # Merchant Name: decimal encoder then one-hot\n",
    "        (\"merchant_name\", Pipeline([\n",
    "            (\"decimal\", FunctionTransformer(decimal_encoder, validate=False)),\n",
    "            (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown='ignore')),\n",
    "        ]), [merchant_name_col]),\n",
    "\n",
    "        # Merchant City: decimal encoder then one-hot\n",
    "        (\"merchant_city\", Pipeline([\n",
    "            (\"decimal\", FunctionTransformer(decimal_encoder, validate=False)),\n",
    "            (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown='ignore')),\n",
    "        ]), [merchant_city_col]),\n",
    "\n",
    "        # Use Chip: impute and binarize\n",
    "        (\"chip\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing_value\")),\n",
    "            (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown='ignore')),\n",
    "        ]), [chip_col]),\n",
    "\n",
    "        # Errors?: impute and binarize\n",
    "        (\"errors\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing_value\")),\n",
    "            (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown='ignore')),\n",
    "        ]), [errors_col]),\n",
    "\n",
    "        # Year/Month/Day/Time: encode and scale\n",
    "        (\"time\", Pipeline([\n",
    "            (\"time_enc\", FunctionTransformer(time_encoder, validate=False)),\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "        ]), time_cols),\n",
    "\n",
    "        # Amount: custom encode and scale\n",
    "        (\"amount\", Pipeline([\n",
    "            (\"amt_enc\", FunctionTransformer(amt_encoder, validate=False)),\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "        ]), [amt_col]),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "processed_array = preprocessor.fit_transform(tdf)\n",
    "\n",
    "# Retrieve feature names\n",
    "\n",
    "feature_names = [f\"feature_{i}\" for i in range(processed_array.shape[1])]\n",
    "\n",
    "print(\"Processed shape:\", processed_array.shape)\n",
    "print(\"First few rows:\\n\", processed_array[:5])\n",
    "print(\"Feature names:\", feature_names)\n",
    "\n",
    "y = processed_array[:, 0]      # Label\n",
    "X = processed_array[:, 1:]     # Features\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "np.savez(\"transactions_processed.npz\", X=X, y=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b8f60",
   "metadata": {},
   "source": [
    "Save all relevant encoding data, to be used by the inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69a78be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models/inference_tx.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib, os, json\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Extract the fitted pieces to match the  ColumnTransformer blocks\n",
    "mn_onehot   = preprocessor.named_transformers_[\"merchant_name\"].named_steps[\"onehot\"]\n",
    "mc_onehot   = preprocessor.named_transformers_[\"merchant_city\"].named_steps[\"onehot\"]\n",
    "chip_imp    = preprocessor.named_transformers_[\"chip\"].named_steps[\"imputer\"]\n",
    "chip_onehot = preprocessor.named_transformers_[\"chip\"].named_steps[\"onehot\"]\n",
    "err_imp     = preprocessor.named_transformers_[\"errors\"].named_steps[\"imputer\"]\n",
    "err_onehot  = preprocessor.named_transformers_[\"errors\"].named_steps[\"onehot\"]\n",
    "time_scaler = preprocessor.named_transformers_[\"time\"].named_steps[\"scaler\"]\n",
    "amt_scaler  = preprocessor.named_transformers_[\"amount\"].named_steps[\"scaler\"]\n",
    "\n",
    "tx_bundle = {\n",
    "    \"merchant_name_onehot\": mn_onehot,\n",
    "    \"merchant_city_onehot\": mc_onehot,\n",
    "    \"chip_imputer\": chip_imp,\n",
    "    \"chip_onehot\": chip_onehot,\n",
    "    \"errors_imputer\": err_imp,\n",
    "    \"errors_onehot\": err_onehot,\n",
    "    \"time_scaler\": time_scaler,\n",
    "    \"amount_scaler\": amt_scaler,\n",
    "\n",
    "    # meta: column names the server will expect\n",
    "    \"columns\": {\n",
    "        \"fraud\": \"Is Fraud?\",\n",
    "        \"merchant_name\": \"Merchant Name\",\n",
    "        \"merchant_city\": \"Merchant City\",\n",
    "        \"chip\": \"Use Chip\",\n",
    "        \"errors\": \"Errors?\",\n",
    "        \"time\": [\"Year\",\"Month\",\"Day\",\"Time\"],\n",
    "        \"amount\": \"Amount\",\n",
    "        \"group_keys\": [\"User\",\"Card\"]  # used to build sequences\n",
    "    },\n",
    "    # the decimal encoder length you used\n",
    "    \"decimal_length\": 5\n",
    "}\n",
    "\n",
    "joblib.dump(tx_bundle, \"models/inference_tx.joblib\")\n",
    "print(\"Saved models/inference_tx.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
